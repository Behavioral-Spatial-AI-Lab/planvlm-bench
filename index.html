<!DOCTYPE html>
<html>
<head>
  <title>PlanVLM-bench</title>
  <meta charset="utf-8">
  <meta name="description"
        content="Multimodal Multi-image Understanding for Evaluating Multimodal Large Language Models">
  <meta name="keywords" content="PlanVLM-bench, LVLM, LVLM Evaluation, multiple images, Vision Language Model, Large Language Model, Large Multimodal Model, artificial intelligence, AI, AGI, artificial general intelligence">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>PlanVLM-bench: Multimodal Multi-image Understanding for Evaluating Multimodal Large Language Models</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./css/bulma.min.css">
  <link rel="stylesheet" href="./css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./css/bulma-slider.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./css/index.css">
  <link rel="stylesheet" href="./css/leaderboard.css">

  <style>
    .hidden {
      display: none;
    }
  </style>
</head>

<body>
<div id="language-switcher" style="position: absolute; top: 1rem; left: 1rem; z-index: 999;">
  <button onclick="changeLanguage('en')" 
          class="external-link button is-normal is-rounded is-dark" 
          style="margin-right: 0.5rem;">
    English
  </button>
  <button onclick="changeLanguage('zh')" 
          class="external-link button is-normal is-rounded is-dark">
    中文
  </button>
</div>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: flex-end;">
      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://github.com">
            <b>PlanVLM</b> <p style="font-size:18px; display: inline; margin-left: 5px;">🔥</p>
          </a>
          <a class="navbar-item" href="https://github.com">
            <b>PlanGPT-Bench</b> <p style="font-size:18px; display: inline; margin-left: 5px;">🔥</p>
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title is-bold">
            <span class="MMIU" style="vertical-align: middle">PlanVLM-bench</span>
            </h1>
          <h2 class="subtitle is-3 publication-subtitle">
            Multimodal Multi-image Understanding for Evaluating Multimodal Large Language Models
          </h2>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Minxin Chen<sup>†,</sup><sup style="color:#6fbf73;">1,</sup><sup style="color:#ffac33;">2</sup></a>
              ,
            </span>
            <span class="author-block">
              He Zhu<sup>†,</sup><sup style="color:#6fbf73;">1,</sup><sup style="color:#ffac33;">2</sup></a>
              ,
            </span>
            <span class="author-block">
              Junyou Su</sup><sup style="color:#6fbf73;">1,</sup><sup style="color:#ffac33;">2</sup></a>
              ,
            </span>
            <span class="author-block">
              Wen Wang</sup><sup style="color:#6fbf73;">1,</sup><sup style="color:#ffac33;">2</sup></a>
              ,
            </span>
            <span class="author-block">
              Yijie Deng</sup><sup style="color:#6fbf73;">1,</sup><sup style="color:#ffac33;">2</sup></a>
              ,
            </span>
            <span class="author-block">
              Wenjia Zhang<sup>*,</sup><sup style="color:#6fbf73;">1,</sup><sup style="color:#ed4b82;">3</sup></a>
            </span>
<!--             <span class="author-block">Quanfeng Lu<sup style="color:#6fbf73;">1</sup><sup>,</sup><sup style="color:#ffac33;">2</sup>,</span>
            <span class="author-block">Hao Tian<sup style="color:#007bff;">4</sup>,</span>
            <span class="author-block">Jiaqi Liao<sup style="color:#6fbf73;">1</sup>,</span>
            <span class="author-block">Xizhou Zhu<sup style="color:#9b51e0;">5</sup><sup>,</sup><sup style="color:#6fbf73;">1</sup><sup>,</sup><sup style="color:#007bff;">4</sup>,</span>
            <span class="author-block">Jifeng Dai<sup style="color:#9b51e0;">5</sup><sup>,</sup><sup style="color:#6fbf73;">1</sup>,</span>
            <span class="author-block">Yu Qiao<sup style="color:#6fbf73;">1</sup>,</span>
            <span class="author-block">
              Ping Luo<sup style="color:#ed4b82;">3</sup><sup>,</sup><sup style="color:#6fbf73;">1</sup></a>
              ,
-->
          
          </div>
          
          <br>
          
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup style="color:#6fbf73;">1</sup>Behavioral and Spatial AI Lab, Tongji University</span>
            <span class="author-block"><sup style="color:#ffac33;">2</sup>Behavioral and Spatial AI Lab, Peking University</span>
            <span class="author-block"><sup style="color:#ed4b82;">3</sup>College of Architecture and Urban Planning, Tongji University</span>
            <!-- <span class="author-block"><sup style="color:#007bff;">4</sup>SenseTime Research,</span>
            <span class="author-block"><sup style="color:#9b51e0;">5</sup>Tsinghua University</span> --> 
          </div>

          <br>
          <div class="is-size-5 publication-authors">
            <span class="author-block">*Corresponding Author:</span>
            <span class="author-block"><a href="mailto:zhangkaipeng@pjlab.org.cn">wenjiazhang@tongji.edu.cn</a></span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block">†Equal Contribution</span>
          </div>
          

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <!-- @PAN TODO: change links -->
                <a href="https://arxiv.org"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://huggingface.co"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <!-- <i class="far fa-images"></i> -->
                      <p style="font-size:18px">🤗</p>
                      <!-- 🔗 -->
                  </span>
                  <span>Dataset</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              
              <!-- Leaderboard Link. -->
              <!-- <span class="link-block">
                <a href="#leaderboard"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon has-text-white">
                    <i class="fa-solid fa-trophy"></i>
                  </span>
                  <span>Leaderboard</span>
                </a>
              </span> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container" style="margin-bottom: 2vh;">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">🔔News</h2>
        <div class="content has-text-justified">
          <p style="color: red>
            <b>[2025-05-20] The arxiv article and dataset along with the code will be launched soon.</a>.</b>
          </p>
      </div>      
        <div class="lang-en">
            <h2 class="title is-3">Introduction</h2>
            <div class="content has-text-justified">
            <p  style="line-height: 2;">
                Planning maps of territorial and spatial planning visually present the concepts, objectives, strategies, and specific measures of territorial and spatial planning in map form, serving to guide and coordinate various development, protection, and utilization activities across territorial space. They not only constitute a critical basis for planning decisions but also act as essential tools for public participation and oversight of plan implementation. Given the complexity and specialized nature of planning work, fully grasping planning maps requires not only grasping fine-grained elements (such as symbols, legends, and geographic features) but also possessing the ability to perform comprehensive analysis and judgment in conjunction with relevant policies. This complexity renders the interpretation of planning maps particularly challenging.With the rapid advancement of multimodal large language models (MLLMs), we have established a benchmark for territorial and spatial planning maps to assess MLLMs’ map-understanding capabilities. Our contributions are as follows:<br> 
                <br>
                <b>(1)	Data:</b> We constructed the Spatial Planning Map Database (SPMD), an expert-annotated repository characterized by diverse map content and high-quality annotations provided by planning domain specialists.<br> 
                <b>(2)	Framework:</b> We proposed a comprehensive, planning-discipline–based evaluation standard that measures MLLMs’ planning-map comprehension from four perspectives—perception, reasoning, association, and application—comprising eight fine-grained subcategories.<br> 
                <b>(3)	Experiments:</b> By designing question–answer tasks grounded in authoritative question banks (specifically, the practice exam questions for the Chinese Registered Urban Planner qualification), we significantly reduced the incidence of hallucinated normative references by the models.<br> 
                <b>(4)	Results:</b> Current state-of-the-art visual large models exhibit significant limitations in the tasks of interpreting and analyzing planning maps. Specifically, these models often fail to accurately recognize the various elements within planning maps and lack the policy-sensitive acuity and professional analytical skills required for planning practice.<br> 
            </p>
            </div>
        </div>
        <div class="lang-zh" style="display: none;">
            <h2 class="title is-3">概述</h2>
            <div class="content has-text-justified" style="line-height: 2;">
            <p>
                国土空间规划图是将国土空间规划的理念、目标、策略和具体措施以地图的形式直观展示出来，用于指导和协调各类国土空间开发、保护和利用活动。它不仅是规划决策的重要依据，也是公众参与和监督规划实施的重要工具。规划是综合性和专业性极强的工作，如果要读透规划图纸，不仅要抓住精细的元素（符号、图例和地理要素等），还要有结合政策进行综合分析和判断的能力。这种复杂性使得规划图的理解具有挑战性。随着多模态大型语言模型（MLLMs）的快速发展，我们建立了国土空间规划图的Benchmark，以评估MLLMs在规划图理解方面的能力。我们的贡献如下：<br>
                <b>(1)	数据：</b> 我们构建了一个专家标注的规划图数据库Spatial Planning Map Database（SPMD)，其特点是多样化的图像内容和由规划领域专家提供的高质量标注。<br>
                <b>(2)	框架：</b> 我们提出了一套基于规划学科的综合标准，从感知、推理、关联、应用四个角度衡量MLLMs的规划图理解能力，包括8个细分类别。<br>
                <b>(3)	实验：</b> 通过基于权威题库（中国注册城市规划师执业资格考试实务题目）知识构建的问答任务，显著降低了模型“幻觉式规范引用”的比例。<br>
                <b>(4)	结果：</b> 目前的SOTA视觉大模型，在解读和分析规划图任务中表现出显著的局限性。具体而言，这些模型常常无法完全准确识别规划图中的各类要素，也缺乏规划所需的政策嗅觉灵敏度和专业分析能力。<br>
            </p>
            </div>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
</div>
</section>

<!-- <section class="hero is-light is-small">
  <div class="hero-body has-text-centered">
  <h1 class="title is-1 MMIU">
    <span class="MMIU" style="vertical-align: middle">MMIU</span>
  </h1>
  </div>
</section> -->

<section class="section">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <!-- <div class="column is-full-width has-text-centered"> -->
      <div class="column is-four-fifths">
        <h2 class="title is-3">Overview</h2>
        <div class="content has-text-justified">
          <img src="./pictures/VLMbench250512.png" alt="pipeline" class="center">
        </div>
        <div class="lang-en">
          <div class="content has-text-justified" style="line-height: 2;">
            <p>
              We propose a conceptual framework tailored to the domain of urban planning visualization.This framework comprises the following 4 dimensions and 8 categories:
              <h3 class="title is-5">Perception</h3>
              The questions of the perception subset consist of 2 categories as follows:<br>
              <b>Element Recognition</b> evaluates models' ability to identify layout configurations, textual annotations, basic geographic features, and drawing elements in planning maps. It facilitates the establishment of semantic alignment between image content and natural language.<br>
              <!-- <img src="./pictures/table1.png" alt="table1" class="center"> -->
              <b>Caption</b>，in this study, refers to extracting as many details from the image as possible. The model generates descriptions based solely on the image itself, rather than identifying elements in response to a specific question. The quality of the caption reflects whether the model has truly "seen" the image clearly.<br>
              Example Question: "Please describe this planning map in detail."
              <h3 class="title is-5">Reasoning</h3>
              This dimension encompasses classification, spatial‐relation reasoning, and domain‐specific reasoning. <br>
              <b>Classification</b> focuses on the ability to recognize different types of planning maps. Based on China's five-level, three-category planning system, the maps are categorized into master plans, detailed plans (including regulatory and site plans), and specialized planning maps.<br>
              <b>Spatial Relationship Reasoning</b>, as shown in Table 2, assesses the understanding of spatial relationships between geographic elements in planning maps, including: topological spatial relations, sequential spatial relations, and metric spatial relations.<br>
              <!-- <img src="./pictures/table2.png" alt="table2" class="center"> -->
              <b>Professional Reasoning</b>, which measures mastery of planning knowledge such as layout morphology, functional organization, transportation systems, and environmental ecology, distinct from general logical inference.
              <h3 class="title is-5">Association</h3>
              This dimension assesses the ability to collect and relate background policies and contextual documents relevant to planning maps. At a fine scale, it examines policy, regulations, and planning indicators.
              <h3 class="title is-5">Implementation</h3>
              This dimension addresses the capacity for comparing, critiquing, and optimizing planning proposals. Given the highly integrative and wide‐ranging nature of urban planning, the ability to identify critical issues and emphasize key priorities is paramount.<br>
              The questions of the Implementation subset consist of 3 categories as follows:<br>
              <b>Task Abstraction:</b> This assesses the ability to identify and extract key information from the question text. In the Certified Urban-Rural Planner Qualification Examination, practical questions often include a long background passage containing irrelevant constraints that need to be filtered and summarized.<br>
              <blockquote>
                Example Question: “A county currently has a population of 980,000, including 520,000 urban residents. By 2035, the plan aims to increase the urbanization rate to 80%. Three central towns—A, B, and C—and the old district of the county seat will undergo upgrading and renovation. Sixty percent of the population will be relocated to newly built residential areas on the outskirts of the county seat. The plan also includes the development of a food industrial park and rural tourism, as shown in the figure. What are the main issues presented in the map? Please extract the key information from the question.”
                <br>
                Example Answer: The key points in this question are urbanization rate, central town, population migration, and food industrial park.
              </blockquote>

              <b>Task-Oriented Image Summarization</b> assesses the ability to identify and extract key information from an image. The image often contains rich information, and some parts are highly relevant to the correct answer and need to be summarized accordingly.<br>
              
              <blockquote>
                Example Question: “Identify the main issues shown in the image.”<br>
                Example Answer: “The main issues in the image include the spatial relationship between Central Town A, the food industrial park, and the floodplain; the topological relationship between the expressway and the nationally protected wetland; the distance between the planned interchange on the west side of the expressway and the existing one; and the overall distribution of central towns.”<br>
              </blockquote>
              </p>
          </div>
        </div>
        <div class="lang-zh" style="display: none;">
          <div class="content has-text-justified" style="line-height: 2;">
            <p>
              我们提出了一个针对国土空间规划可视化领域的概念框架。该框架包括以下4个维度和8个类别：
              <h3 class="title is-5">感知</h3>
              感知维度中的问题包括以下2个类别：<br>
              <b>元素识别</b>评估模型对规划图上图幅配置、文字、基础地理要素、图纸要素的识别能力。它有助于模型建立图像内容与自然语言之间的语义映射。<br>
              <!-- <img src="./pictures/table1.png" alt="table1" class="center"> -->
              <b>描述</b>，在本研究中指扒出图像中尽可能多的细节。模型仅基于图像本身，而不是基于问题去从图中定位要素。Caption 的质量可以反映模型是否真正“看清”了图像。<br>
              示例问题：“请详细描述这张规划图。”
              <h3 class="title is-5">推理</h3>
              这个维度包括分类、空间关系推理和专业推理。<br>
              <b>分类</b>关注模型对不同类型的规划图的识别能力。目前按中国的五级三类体系，分为总体规划，详细规划（控制性、修建性）和专项规划图。<br>
              <b>空间关系推理</b>，评估规划图中地理元素之间的空间关系理解，包括：拓扑空间关系、顺序空间关系和度量空间关系。<br>
              <!-- <img src="./pictures/table2.png" alt="table2" class="center"> -->
              <b>专业推理</b>，考察规划图专业知识掌握的能力。包括布局形态、功能组织、交通体系、 环境生态等，区别于常规逻辑推断。
              <h3 class="title is-5">关联</h3>
              该维度评估收集过程中指与规划图相关的背景政策、文件上下文的问题。小尺度注规真题：政策、规范、规划指标。
              <h3 class="title is-5">应用</h3>
              该维度涉及比较、批评和优化规划方案的能力。由于城市规划的高度综合性和广泛性，识别关键问题和强调重点优先事项的能力至关重要。
              应用维度中的问题包括以下3个类别：<br>
              <b>题目归纳</b> 考察识别归纳提取题目文本重点的能力。注册规划师实务考试题目通常会提供一段背景信息的长文本，有一些和答案无关的约束条件，需要进行过滤和归纳。<br>
              <blockquote>
                示例问题：“一个县目前有98万人口，其中52万人为城市居民。到2035年，计划将城市化率提高到80%。三个中心城镇A、B和C以及县城的旧区将进行升级和改造。60%的人口将迁移到县城郊区新建的住宅区。该计划还包括建设食品工业园区和乡村旅游，如图所示。图中主要问题是什么？请提取问题中的关键信息。”
                <br>
                示例答案：这个问题的关键点是城市化率、中心城镇、人口迁移和食品工业园区。
              </blockquote>
              <b>题图归纳</b>考察识别归纳提取图中重点的能力,图中承载着丰富的信息，有一些和答案强相关的部分，需要进行归纳。<br>
              <blockquote>
                示例问题：“提取图片中存在的主要问题。”<br>
                示例答案：“图片中的主要问题是中心镇A、食品产业园与河堤行洪的关系；高速公路与国家重要湿地的拓扑关系；规划高速西侧立交口与现状高速立交口的距离；中心镇分布情况。”<br>
              </blockquote>
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <!-- <div class="column is-full-width has-text-centered"> -->
      <div class="column is-four-fifths">
        <h2 class="title is-3">Performance</h2>
        <div class="content has-text-justified">
          <img src="./pictures/radar_image.png" alt="radar_image" class="center">
        </div>
      </div>
    </div>
  </div>
</section>

<style>
  .image-row {
      display: center; /* Use flexbox layout */
  }
  .image-row img {
      width: 40%; /* Each image takes up 50% of the width */
  }
</style>

<div class="container mt-6">
  <div class="content">
    <!-- 英文版（默认显示） -->
    <div class="lang-en">
      <h2 class="title is-4">Acknowledgements</h2>
      <p><strong>VQA Data Annotators:</strong> Siqi Zha, Yeyang Fu, Chuang Deng, Fenghong An, Hanying Li, Jiayi Fan</p>
      <p><strong>Internal Testers:</strong> Jialu Yu, Yingqi Guo</p>
      <br>
    </div>
    <!-- 中文版（默认隐藏） -->
    <div class="lang-zh" style="display: none;">
      <h2 class="title is-4">致谢</h2>
      <p><strong>VQA数据标注人员：</strong>查思齐，付叶扬，邓闯，安枫泓，李瀚滢，范佳挹</p>
      <p><strong>内测人员：</strong>于佳璐，郭瑛琦</p>
      <br>
    </div>
  </div>
</div>

<section class="section">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <p style="color: red; font-weight: bold;">
          Note: This content is part of a manuscript under submission. Please do not cite until it is officially published. The arxiv article and dataset along with the code will be launched soon.
        </p>
      </div>
    </div>
  </div>
</section>

<!-- @PAN TODO: bibtex -->
<!-- <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title is-3 has-text-centered">BibTeX</h2>
    <pre><code>
      @misc{meng2024mmiumultimodalmultiimageunderstanding,
        title={MMIU: Multimodal Multi-image Understanding for Evaluating Large Vision-Language Models}, 
        author={Fanqing Meng and Jin Wang and Chuanhao Li and Quanfeng Lu and Hao Tian and Jiaqi Liao and Xizhou Zhu and Jifeng Dai and Yu Qiao and Ping Luo and Kaipeng Zhang and Wenqi Shao},
        year={2024},
        eprint={2408.02718},
        archivePrefix={arXiv},
        primaryClass={cs.CV},
        url={https://arxiv.org/abs/2408.02718}, 
  }
</code></pre>
  </div>
</section> -->

<!-- <footer class="footer">
    <div class="content has-text-centered">
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is website adapted from <a href="https://github.com/OpenGVLab/MMT-Bench">MMT-Bench</a>, <a href="https://mmmu-benchmark.github.io/">MMMU</a>, <a href="https://mathvista.github.io/">MathVista</a> and <a href="https://nerfies.github.io/">Nerfies</a>, licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>

</footer> -->

<script>
    function changeLanguage(language) {
        // Hide all language-specific content
        var enContent = document.querySelectorAll('.lang-en');
        var zhContent = document.querySelectorAll('.lang-zh');
        
        if (language === 'en') {
            enContent.forEach(function (el) {
                el.style.display = 'block';
            });
            zhContent.forEach(function (el) {
                el.style.display = 'none';
            });
        } else if (language === 'zh') {
            zhContent.forEach(function (el) {
                el.style.display = 'block';
            });
            enContent.forEach(function (el) {
                el.style.display = 'none';
            });
        }
    }
</script>

<style>
  .hidden {
      display: none;
  }
  .sortable:hover {
      cursor: pointer;
  }
  .asc::after {
      content: ' ↑';
  }
  .desc::after {
      content: ' ↓';
  }

  .center {
    display: block;
    margin-left: auto;
    margin-right: auto;
    width: 80%;
  }

  .publication-links .link-block {
    margin: 0 0.5rem;
    display: inline-block;
  }
  #toggleButton {
    background-color: #ffffff;
    border: 1px solid #dddddd;
    color: #555555;
    padding: 10px 20px;
    text-align: center;
    text-decoration: none;
    display: inline-block;
    font-size: 14px;
    margin: 4px 2px;
    cursor: pointer;
    border-radius: 25px; 
    box-shadow: 0 4px 8px 0 rgba(0,0,0,0.2);
    transition-duration: 0.4s;
  }

  #toggleButton:hover {
    box-shadow: 0 12px 16px 0 rgba(0,0,0,0.24), 0 17px 50px 0 rgba(0,0,0,0.19); /* 鼠标悬停时的阴影效果 */
  }

  table {
    border-collapse: collapse;
    width: 100%;
    margin-top: 5px;
    border: 1px solid #ddd;
    font-size: 14px;
  }

  th, td {
      text-align: left;
      padding: 8px;
  }

  th {
      background-color: #f2f2f2;
      border-bottom: 2px solid #ddd;
  }

  td:hover {background-color: #ffffff;}
</style>
</body>
</html>
